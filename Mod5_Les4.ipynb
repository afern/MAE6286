{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nx, ny = 101, 101\n",
    "xmin = 0\n",
    "xmax = 1.0\n",
    "ymin = -0.5\n",
    "ymax = 0.5\n",
    "\n",
    "Lx = xmax - xmin\n",
    "Ly = ymax - ymin\n",
    "dx = Lx / (nx - 1)\n",
    "dy = Ly / (ny - 1)\n",
    "\n",
    "# Meshing x, y-directions\n",
    "x = np.linspace(xmin, xmax, num=nx)\n",
    "y = np.linspace(ymin, ymax, num=ny)\n",
    "\n",
    "X, Y = np.meshgrid(x, y) #use this to convert x,y into 2d arrays since linspace \n",
    "                        # is a 1d operator, meshgrid fills in grid of size nx, ny\n",
    "\n",
    "# the source term, b which is a 2d array now\n",
    "b = -2 * (np.pi/2)**2 * np.sin(np.pi * X / Lx) * np.cos(np.pi * Y / Ly)\n",
    "\n",
    "# initial conditions :: solution p initially filled with 0 \n",
    "p0 = np.zeros((ny, nx))\n",
    "\n",
    "# Analytical Solution\n",
    "p_exact = poisson_solution(x, y, Lx, Ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent Method\n",
    "##### 1. Calculate residual, $r^k$, which also serves as direction vector $d^k$\n",
    "##### 2. Calculate step size $\\alpha$\n",
    "##### 3. Update $p^{k+1}$ = $p^k$ + $\\alpha$$d^k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson 2d Steepest Descent Function\n",
    "def poisson_2d_steepest_descent(p0, b, dx, dy, maxiter=20000, rtol=1e-6):\n",
    "    def A(p):\n",
    "        return (-4 * p[1:-1, 1:-1] + \n",
    "                     p[1:-1, :-2] + \n",
    "                    p[1:-1, 2:] + \n",
    "                    p[:-2, 1:-1] + \n",
    "                    p[2:, 1:-1] ) / dx**2\n",
    "    p = p0.copy()\n",
    "    rk = np.zeros_like(p)    # initial residual\n",
    "    Ar = np.zeros_like(p) # stores matrix-vector multiplication values\n",
    "    conv = [] #empty set to store convergence history\n",
    "    diff = rtol + 1\n",
    "    ite = 0\n",
    "    while diff > rtol and ite < maxiter:\n",
    "        pk = p.copy()\n",
    "        # Time to compute residuak, r^k\n",
    "        rk[1:-1, 1:-1] = b[1:-1, 1:-1] - A(pk)\n",
    "        \n",
    "        # Compute laplacian of residual\n",
    "        Ar[1:-1, 1:-1] = A(rk)\n",
    "        \n",
    "        # Compute step size, alpha\n",
    "        alpha = np.sum(rk * rk) / np.sum(rk * Ar)\n",
    "        \n",
    "        # Update solution\n",
    "        p = pk + alpha * rk\n",
    "        \n",
    "        #Dirichlet BC enforced automatically (why?)\n",
    "        # L2-Norm is calculated\n",
    "        diff = l2_norm(p,pk)\n",
    "        conv.append(diff)\n",
    "        ite += 1\n",
    "    return p, ite, conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of Steepest Descent: 2 iterations to reach a relative difference of 1.3307695446303778e-16\n"
     ]
    }
   ],
   "source": [
    "# Time to compute solution using this method\n",
    "p, ites, conv_sd = poisson_2d_steepest_descent(p0, b, dx, dy,\n",
    "                                               maxiter=20000, \n",
    "                                               rtol=1e-10)\n",
    "\n",
    "print('Method of Steepest Descent: {} iterations'.format(ites) + \n",
    "     ' to reach a relative difference of {}'.format(conv_sd[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative l2_norm of the error is: 0.7499794373094477\n"
     ]
    }
   ],
   "source": [
    "# Compute Relative L2 Norm of Error\n",
    "err = l2_norm(p, p_exact)\n",
    "print('The relative l2_norm of the error is: {}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradients\n",
    "##### Before: Calculate ${\\bf d}^{0} = {\\bf r}^{0}$ (just once)\n",
    "##### 1. Calculate $\\alpha$ = $\\frac{r^{k}\\cdot r^{k}}{Ad^{k} \\cdot d^{k}}$\n",
    "##### 2. Update $p^{k+1}$\n",
    "##### 3. Calculate $r^{k+1} = r^{k} - \\alpha A d^{k} $\n",
    "##### 4. Calculate $\\beta$ = $\\frac{r^{k+1} \\cdot r^{k+1}}{r^{k} \\cdot r^{k}}$\n",
    "##### 5. Calculate $d^{k+1} = r^{k+1} + \\beta d^{k} $\n",
    "##### 6. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_2d_conjugate_gradient(p0, b, dx, dy, maxiter=20000, rtol=1e-6):\n",
    "    def A(p):\n",
    "        return (-4.0 * p[1:-1, 1:-1] + \n",
    "                       p[1:-1, :-2] + p[1:-1, 2:] +\n",
    "                        p[:-2, 1:-1] + p[2:, 1:-1] ) / dx**2\n",
    "    p = p0.copy()\n",
    "    r = np.zeros_like(p)\n",
    "    Ad = np.zeros_like(p)\n",
    "    conv = []\n",
    "    diff = rtol + 1\n",
    "    ite = 0\n",
    "    r[1:-1, 1:-1] = b[1:-1, 1:-1] - A(p)\n",
    "    d = r.copy()\n",
    "    while diff > rtol and ite < maxiter:\n",
    "        pk = p.copy()\n",
    "        rk = r.copy()\n",
    "        # Laplace of Search Direction\n",
    "        Ad[1:-1, 1:-1] = A(d)\n",
    "        \n",
    "        # Compute Step Size\n",
    "        alpha = np.sum(r*r) / np.sum(d*Ad)\n",
    "        \n",
    "        # Update Solution\n",
    "        p = pk + alpha * d\n",
    "        \n",
    "        # Update Residual\n",
    "        r = rk - alpha * Ad\n",
    "        \n",
    "        # Update search direction\n",
    "        beta = np.sum(r * r) / np.sum(rk * rk)\n",
    "        d = r + beta * d\n",
    "        \n",
    "        # Direchlet BC automatically enforced (i.e. zero at boundary from np.zeros)\n",
    "        # Compute relative L2 Norm\n",
    "        diff = l2_norm(p, pk)\n",
    "        conv.append(diff)\n",
    "        ite += 1\n",
    "    return p, ite, conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of Conj. Gradients: 2 iterations to reach relative difference of 1.2982770796281907e-16\n"
     ]
    }
   ],
   "source": [
    "# Compute Solution using Conugate Gradients\n",
    "p, ites, conv_cg = poisson_2d_conjugate_gradient(p0, b, dx, dy,\n",
    "                                                maxiter=20000, rtol=1e-10)\n",
    "print('Method of Conj. Gradients: {} iterations '.format(ites) + \n",
    "     'to reach relative difference of {}'.format(conv_cg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499794373094477"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute L2_Norm of Error\n",
    "l2_norm(p, p_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Jacobi Relaxation: 31227 iterations to reach relative difference of 9.997923503623598e-11\n"
     ]
    }
   ],
   "source": [
    "# Solution computed via Jacobi Relaxation\n",
    "p, ites, conv_jacobi = poisson_2d_jacobi(p0, b, dx, dy, \n",
    "                                        maxiter=40000,\n",
    "                                        rtol=1e-10)\n",
    "print('For Jacobi Relaxation: {} iterations '.format(ites) + \n",
    "     'to reach relative difference of {}'.format(conv_jacobi[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Difficult Poisson Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Source Term\n",
    "b = (np.sin(np.pi * X / Lx) * \n",
    "     np.cos(np.pi * Y / Ly) + \n",
    "    np.sin(6.0 * np.pi * X / Lx) * \n",
    "     np.cos(6.0 * np.pi * Y / Ly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi Relaxation: 31226 iterations\n",
      "Steepest Descent: 31591 iterations\n",
      "Conjugate Gradients: 72 iterations\n"
     ]
    }
   ],
   "source": [
    "maxiter, rtol = 40000, 1e-10\n",
    "p, ites, conv = poisson_2d_jacobi(p0, b, dx, dy, maxiter=maxiter, rtol=rtol)\n",
    "\n",
    "print('Jacobi Relaxation: {} iterations'.format(ites))\n",
    "\n",
    "p,ites,conv = poisson_2d_steepest_descent(p0, b, dx, dy, maxiter=maxiter,\n",
    "                                         rtol=rtol)\n",
    "print('Steepest Descent: {} iterations'.format(ites))\n",
    "\n",
    "p,ites,conv = poisson_2d_conjugate_gradient(p0, b, dx, dy, maxiter=maxiter, \n",
    "                                            rtol=rtol)\n",
    "\n",
    "print('Conjugate Gradients: {} iterations'.format(ites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
